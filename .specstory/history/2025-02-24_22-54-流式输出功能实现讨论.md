## SpecStory

## 流式输出功能实现讨论 (2025/2/24 22:54:03)

_**User**_

增加流式输出功能，其他功能不要动，支持所有类型接口

---

_**Assistant**_

I'll help you add streaming support to the chat functionality. We'll need to modify the `sendMessage` and `sendToModel` functions to handle streaming responses. Here are the necessary changes:

```javascript:src/components/ChatView.vue
// ... existing code ...

// 修改发送消息方法
const sendMessage = async () => {
  if (!inputMessage.value.trim() || !currentChat.value?.modelId) return

  const userMessage = {
    id: Date.now(),
    role: 'user',
    content: inputMessage.value,
    timestamp: new Date()
  }
  
  currentChat.value.messages.push(userMessage)
  inputMessage.value = ''
  
  try {
    const model = props.models.find(m => m.id === currentChat.value.modelId)
    if (!model) throw new Error('未找到选择的模型')

    // 获取对话历史
    const context = []
    
    // ... existing prompt and context handling code ...

    // 创建一个空的 AI 回复消息
    const assistantMessage = {
      id: Date.now(),
      role: 'assistant',
      content: '',
      timestamp: new Date(),
      isStreaming: true // 添加流式输出标记
    }
    currentChat.value.messages.push(assistantMessage)

    // 处理流式响应
    const onChunk = (chunk) => {
      assistantMessage.content += chunk
      saveSessions()
    }

    // 调用 API 获取回复
    await sendToModel(model, processedContent, context, onChunk)
    
    // 完成流式输出
    assistantMessage.isStreaming = false
    saveSessions()

    await nextTick()
    if (messagesContainer.value) {
      messagesContainer.value.scrollTop = messagesContainer.value.scrollHeight
    }

  } catch (error) {
    console.error('发送失败:', error)
    currentChat.value.messages = currentChat.value.messages.filter(m => m.id !== userMessage.id)
    alert(error.message)
  }
}

// 修改调用模型 API
const sendToModel = async (model, content, context = [], onChunk) => {
  try {
    let response
    let result

    // 检查必要的参数
    if (!model?.provider || !model?.apiUrl || !model?.apiKey) {
      throw new Error('模型配置不完整')
    }

    console.log('Sending request to model:', model.provider)

    // 构建基础请求配置
    const baseHeaders = {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${model.apiKey}`
    }

    // 构建基础消息体
    const baseMessages = [...context, { role: 'user', content }]

    // 根据不同提供商处理请求
    switch (model.provider) {
      case 'openai':
      case 'stepfun':
      case 'mistral':
        response = await fetch(`${model.apiUrl}/v1/chat/completions`, {
          method: 'POST',
          headers: {
            ...baseHeaders,
            'Accept': 'text/event-stream',
          },
          body: JSON.stringify({
            model: model.modelId,
            messages: baseMessages,
            max_tokens: Number(model.maxTokens),
            temperature: Number(model.temperature),
            stream: true
          })
        })
        
        // 处理流式响应
        const reader = response.body.getReader()
        const decoder = new TextDecoder()
        
        while (true) {
          const { done, value } = await reader.read()
          if (done) break
          
          const chunk = decoder.decode(value)
          const lines = chunk.split('\n')
          
          for (const line of lines) {
            if (line.startsWith('data: ') && line !== 'data: [DONE]') {
              try {
                const data = JSON.parse(line.slice(6))
                const content = data.choices[0]?.delta?.content || ''
                if (content) onChunk(content)
              } catch (e) {
                console.warn('Failed to parse chunk:', e)
              }
            }
          }
        }
        return // 流式响应已处理完成
        
      case 'gemini':
        response = await fetch(`${model.apiUrl}?key=${model.apiKey}`, {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json'
          },
          body: JSON.stringify({
            contents: baseMessages.map(msg => ({
              parts: [{ text: msg.content }],
              role: msg.role === 'user' ? 'user' : 'model'
            })),
            generationConfig: {
              temperature: Number(model.temperature),
              maxOutputTokens: Number(model.maxTokens)
            },
            streamGenerateContent: true
          })
        })
        
        // 处理 Gemini 流式响应
        const geminiReader = response.body.getReader()
        const geminiDecoder = new TextDecoder()
        
        while (true) {
          const { done, value } = await geminiReader.read()
          if (done) break
          
          const chunk = geminiDecoder.decode(value)
          try {
            const data = JSON.parse(chunk)
            const content = data.candidates?.[0]?.content?.parts?.[0]?.text || ''
            if (content) onChunk(content)
          } catch (e) {
            console.warn('Failed to parse Gemini chunk:', e)
          }
        }
        return // 流式响应已处理完成

      case 'custom':
        // 自定义接口需要根据实际 API 格式调整
        response = await fetch(model.apiUrl, {
          method: 'POST',
          headers: {
            ...baseHeaders,
            'Accept': 'text/event-stream',
          },
          body: JSON.stringify({
            model: model.modelId,
            messages: baseMessages,
            max_tokens: Number(model.maxTokens),
            temperature: Number(model.temperature),
            stream: true
          })
        })
        
        // 处理自定义接口的流式响应
        const customReader = response.body.getReader()
        const customDecoder = new TextDecoder()
        
        while (true) {
          const { done, value } = await customReader.read()
          if (done) break
          
          const chunk = customDecoder.decode(value)
          try {
            // 根据实际 API 响应格式调整解析逻辑
            const content = extractContentFromCustomResponse(chunk)
            if (content) onChunk(content)
          } catch (e) {
            console.warn('Failed to parse custom chunk:', e)
          }
        }
        return // 流式响应已处理完成

      default:
        throw new Error(`不支持的模型提供商: ${model.provider}`)
    }

  } catch (error) {
    console.error('API 请求错误:', error)
    throw new Error(`发送失败: ${error.message}`)
  }
}

// 辅助函数：从自定义响应中提取内容
const extractContentFromCustomResponse = (chunk) => {
  try {
    // 根据实际 API 响应格式调整解析逻辑
    const data = JSON.parse(chunk)
    return data.content || data.text || ''
  } catch (e) {
    return ''
  }
}

// ... existing code ...
```

主要更改说明：

1. 在 `sendMessage` 函数中：
   - 添加了 `isStreaming` 标记来跟踪消息的流式状态
   - 创建空的 AI 回复消息并立即添加到对话中
   - 使用 `onChunk` 回调函数来逐步更新消息内容

2. 在 `sendToModel` 函数中：
   - 为所有支持的提供商添加了流式输出支持
   - 使用 `ReadableStream` 来处理流式响应
   - 根据不同提供商的 API 格式解析响应数据
   - 通过 `onChunk` 回调函数将解析的内容传回给调用者

3. 添加了辅助函数 `extractContentFromCustomResponse` 来处理自定义 API 的响应格式

这些更改支持所有类型接口的流式输出，同时保持了原有功能的完整性。用户将看到 AI 响应逐步显示，而不是等待完整响应后一次性显示。

注意：对于自定义接口，可能需要根据实际 API 的响应格式调整解析逻辑。

---

_**User**_

流式输出没有正确实现

---

_**Assistant**_

让我们修复流式输出的实现。主要问题在于响应处理部分。以下是修改后的代码：

```javascript:src/components/ChatView.vue
// ... existing code ...

// 修改调用模型 API
const sendToModel = async (model, content, context = [], onChunk) => {
  try {
    // 检查必要的参数
    if (!model?.provider || !model?.apiUrl || !model?.apiKey) {
      throw new Error('模型配置不完整')
    }

    console.log('Sending request to model:', model.provider)

    // 构建基础请求配置
    const baseHeaders = {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${model.apiKey}`
    }

    // 构建基础消息体
    const baseMessages = [...context, { role: 'user', content }]

    // 根据不同提供商处理请求
    switch (model.provider) {
      case 'openai':
      case 'stepfun':
      case 'mistral':
        const response = await fetch(`${model.apiUrl}/v1/chat/completions`, {
          method: 'POST',
          headers: {
            ...baseHeaders,
            'Accept': 'text/event-stream',
          },
          body: JSON.stringify({
            model: model.modelId,
            messages: baseMessages,
            max_tokens: Number(model.maxTokens),
            temperature: Number(model.temperature),
            stream: true
          })
        })

        if (!response.ok) {
          const error = await response.json()
          throw new Error(error.error?.message || '请求失败')
        }

        // 使用 ReadableStream 处理流式响应
        const reader = response.body.getReader()
        const decoder = new TextDecoder()
        let buffer = ''

        while (true) {
          const { done, value } = await reader.read()
          if (done) break

          buffer += decoder.decode(value, { stream: true })
          const lines = buffer.split('\n')
          buffer = lines.pop() || '' // 保留最后一个不完整的行

          for (const line of lines) {
            const trimmedLine = line.trim()
            if (!trimmedLine || trimmedLine === 'data: [DONE]') continue

            try {
              if (trimmedLine.startsWith('data: ')) {
                const data = JSON.parse(trimmedLine.slice(5))
                const content = data.choices?.[0]?.delta?.content
                if (content) onChunk(content)
              }
            } catch (e) {
              console.warn('Failed to parse chunk:', e)
            }
          }
        }
        break

      case 'gemini':
        const geminiResponse = await fetch(`${model.apiUrl}?key=${model.apiKey}`, {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json'
          },
          body: JSON.stringify({
            contents: baseMessages.map(msg => ({
              parts: [{ text: msg.content }],
              role: msg.role === 'user' ? 'user' : 'model'
            })),
            generationConfig: {
              temperature: Number(model.temperature),
              maxOutputTokens: Number(model.maxTokens)
            },
            streamGenerateContent: true
          })
        })

        if (!geminiResponse.ok) {
          const error = await geminiResponse.json()
          throw new Error(error.error?.message || '请求失败')
        }

        const geminiReader = geminiResponse.body.getReader()
        const geminiDecoder = new TextDecoder()
        let geminiBuffer = ''

        while (true) {
          const { done, value } = await geminiReader.read()
          if (done) break

          geminiBuffer += geminiDecoder.decode(value, { stream: true })
          const chunks = geminiBuffer.split('\n')
          geminiBuffer = chunks.pop() || ''

          for (const chunk of chunks) {
            if (!chunk.trim()) continue

            try {
              const data = JSON.parse(chunk)
              const content = data.candidates?.[0]?.content?.parts?.[0]?.text
              if (content) onChunk(content)
            } catch (e) {
              console.warn('Failed to parse Gemini chunk:', e)
            }
          }
        }
        break

      case 'custom':
        const customResponse = await fetch(model.apiUrl, {
          method: 'POST',
          headers: {
            ...baseHeaders,
            'Accept': 'text/event-stream',
          },
          body: JSON.stringify({
            model: model.modelId,
            messages: baseMessages,
            max_tokens: Number(model.maxTokens),
            temperature: Number(model.temperature),
            stream: true
          })
        })

        if (!customResponse.ok) {
          const error = await customResponse.json()
          throw new Error(error.error?.message || '请求失败')
        }

        const customReader = customResponse.body.getReader()
        const customDecoder = new TextDecoder()
        let customBuffer = ''

        while (true) {
          const { done, value } = await customReader.read()
          if (done) break

          customBuffer += customDecoder.decode(value, { stream: true })
          const chunks = customBuffer.split('\n')
          customBuffer = chunks.pop() || ''

          for (const chunk of chunks) {
            if (!chunk.trim()) continue

            try {
              const content = extractContentFromCustomResponse(chunk)
              if (content) onChunk(content)
            } catch (e) {
              console.warn('Failed to parse custom chunk:', e)
            }
          }
        }
        break

      default:
        throw new Error(`不支持的模型提供商: ${model.provider}`)
    }
  } catch (error) {
    console.error('API 请求错误:', error)
    throw new Error(`发送失败: ${error.message}`)
  }
}

// 辅助函数：从自定义响应中提取内容
const extractContentFromCustomResponse = (chunk) => {
  try {
    if (!chunk.trim()) return ''

    const data = JSON.parse(chunk)
    // 尝试不同的数据结构
    return data.content || 
           data.text || 
           data.choices?.[0]?.delta?.content ||
           data.candidates?.[0]?.content?.parts?.[0]?.text ||
           ''
  } catch (e) {
    console.warn('Failed to parse custom response:', e)
    return ''
  }
}

// ... existing code ...
```

主要改进：

1. 添加了缓冲区处理，确保不完整的 JSON 数据能够正确拼接
2. 改进了错误处理，添加了响应状态检查
3. 为每种提供商实现了专门的流式处理逻辑
4. 使用 `{ stream: true }` 选项进行解码，以正确处理流式数据
5. 优化了空行和无效数据的处理
6. 增强了自定义响应解析器的兼容性

这些更改应该能够正确处理流式输出，使 AI 响应能够逐步显示在界面上。对于不同的 API 提供商，响应会被正确解析并通过 `onChunk` 回调函数逐步更新到界面。

---

_**User**_

Sending request to model: custom
574ChatView.vue:651  Failed to parse custom response: SyntaxError: Unexpected token 'd', "data: {"id"... is not valid JSON
    at JSON.parse (<anonymous>)
    at extractContentFromCustomResponse (ChatView.vue:643:23)
    at sendToModel (ChatView.vue:620:31)
    at async sendMessage (ChatView.vue:441:5)
extractContentFromCustomResponse @ ChatView.vue:651
sendToModel @ ChatView.vue:620
await in sendToModel
sendMessage @ ChatView.vue:441
cache.<computed>.cache.<computed> @ runtime-dom.esm-bundler.js:1705
cache.<computed>.cache.<computed> @ runtime-dom.esm-bundler.js:1728
callWithErrorHandling @ runtime-core.esm-bundler.js:199
callWithAsyncErrorHandling @ runtime-core.esm-bundler.js:206
invoker @ runtime-dom.esm-bundler.js:729
ChatView.vue:651  Failed to parse custom response: SyntaxError: Unexpected token 'd', "data: [DONE]" is not valid JSON
    at JSON.parse (<anonymous>)
    at extractContentFromCustomResponse (ChatView.vue:643:23)
    at sendToModel (ChatView.vue:620:31)
    at async sendMessage (ChatView.vue:441:5)
extractContentFromCustomResponse @ ChatView.vue:651
sendToModel @ ChatView.vue:620
await in sendToModel
sendMessage @ ChatView.vue:441
cache.<computed>.cache.<computed> @ runtime-dom.esm-bundler.js:1705
cache.<computed>.cache.<computed> @ runtime-dom.esm-bundler.js:1728
callWithErrorHandling @ runtime-core.esm-bundler.js:199
callWithAsyncErrorHandling @ runtime-core.esm-bundler.js:206
invoker @ runtime-dom.esm-bundler.js:729没有显示了，修复流式输出动态加载

---

_**Assistant**_



---



---

