## SpecStory

## 流式输出实现讨论 (2025/2/25 08:22:46)

_**User**_

更新流式输出

---

_**Assistant**_

I'll help you implement streaming output for the chat interface. This will make the AI responses appear more natural by showing them character by character as they're generated, rather than waiting for the complete response.

Here are the necessary changes:

```vue:src/components/ChatView.vue
// ... existing code ...
// 修改调用模型 API
const sendToModel = async (model, content, context = []) => {
  try {
    let response
    let result

    // 检查必要的参数
    if (!model?.provider || !model?.apiUrl || !model?.apiKey) {
      throw new Error('模型配置不完整')
    }

    console.log('Sending request to model:', model.provider)

    // 构建基础请求配置
    const baseHeaders = {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${model.apiKey}`
    }

    // 构建基础消息体
    const baseMessages = [...context, { role: 'user', content }]

    // 创建一个临时消息用于流式输出
    const tempMessage = {
      id: Date.now(),
      role: 'assistant',
      content: '',
      timestamp: new Date(),
      isStreaming: true
    }
    currentChat.value.messages.push(tempMessage)

    // 根据不同提供商处理请求
    switch (model.provider) {
      case 'openai':
      case 'stepfun':
      case 'mistral':
        response = await fetch(`${model.apiUrl}/v1/chat/completions`, {
          method: 'POST',
          headers: baseHeaders,
          body: JSON.stringify({
            model: model.modelId,
            messages: baseMessages,
            max_tokens: Number(model.maxTokens),
            temperature: Number(model.temperature),
            stream: true // 启用流式输出
          })
        })
        
        if (!response.ok) {
          const errorData = await response.json().catch(() => ({}))
          throw new Error(`请求失败: ${response.status} ${errorData.error?.message || ''}`)
        }
        
        const reader = response.body.getReader()
        const decoder = new TextDecoder("utf-8")
        
        while (true) {
          const { done, value } = await reader.read()
          if (done) break
          
          const chunk = decoder.decode(value)
          const lines = chunk.split('\n').filter(line => line.trim() !== '')
          
          for (const line of lines) {
            if (line.startsWith('data: ')) {
              const data = line.substring(6)
              if (data === '[DONE]') continue
              
              try {
                const parsed = JSON.parse(data)
                const content = parsed.choices[0]?.delta?.content || ''
                if (content) {
                  tempMessage.content += content
                  await nextTick()
                  scrollToBottom()
                }
              } catch (e) {
                console.error('解析流数据失败:', e, data)
              }
            }
          }
        }
        
        // 流式输出完成后
        tempMessage.isStreaming = false
        saveSessions()
        return tempMessage.content
        
      case 'custom': // 直接使用提供的 URL，不附加路径
        response = await fetch(model.apiUrl, {
          method: 'POST',
          headers: baseHeaders,
          body: JSON.stringify({
            model: model.modelId,
            messages: baseMessages,
            max_tokens: Number(model.maxTokens),
            temperature: Number(model.temperature),
            stream: true // 启用流式输出
          })
        })
        
        if (!response.ok) {
          const errorData = await response.json().catch(() => ({}))
          throw new Error(`请求失败: ${response.status} ${errorData.error?.message || ''}`)
        }
        
        const customReader = response.body.getReader()
        const customDecoder = new TextDecoder("utf-8")
        
        while (true) {
          const { done, value } = await customReader.read()
          if (done) break
          
          const chunk = customDecoder.decode(value)
          const lines = chunk.split('\n').filter(line => line.trim() !== '')
          
          for (const line of lines) {
            if (line.startsWith('data: ')) {
              const data = line.substring(6)
              if (data === '[DONE]') continue
              
              try {
                const parsed = JSON.parse(data)
                const content = parsed.choices[0]?.delta?.content || ''
                if (content) {
                  tempMessage.content += content
                  await nextTick()
                  scrollToBottom()
                }
              } catch (e) {
                console.error('解析流数据失败:', e, data)
              }
            }
          }
        }
        
        // 流式输出完成后
        tempMessage.isStreaming = false
        saveSessions()
        return tempMessage.content

      case 'gemini':
        const contents = context.map(msg => ({
          parts: [{ text: msg.content }],
          role: msg.role === 'user' ? 'user' : 'model'
        }))
        
        contents.push({
          parts: [{ text: content }],
          role: 'user'
        })

        // Gemini 目前不支持流式输出，使用普通请求
        response = await fetch(`${model.apiUrl}?key=${model.apiKey}`, {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json'
          },
          body: JSON.stringify({
            contents,
            generationConfig: {
              temperature: Number(model.temperature),
              maxOutputTokens: Number(model.maxTokens)
            }
          })
        })
        
        if (!response.ok) {
          const errorData = await response.json().catch(() => ({}))
          throw new Error(`请求失败: ${response.status} ${errorData.error?.message || ''}`)
        }
        
        result = await response.json()
        tempMessage.content = result.candidates[0].content.parts[0].text
        tempMessage.isStreaming = false
        saveSessions()
        return tempMessage.content

      default:
        throw new Error(`不支持的模型提供商: ${model.provider}`)
    }

  } catch (error) {
    console.error('API 请求错误:', error)
    // 移除临时消息
    currentChat.value.messages = currentChat.value.messages.filter(m => !m.isStreaming)
    throw new Error(`发送失败: ${error.message}`)
  }
}

// 修改发送消息方法
const sendMessage = async () => {
  if (!inputMessage.value.trim() || !currentChat.value?.modelId) return

  const userMessage = {
    id: Date.now(),
    role: 'user',
    content: inputMessage.value,
    timestamp: new Date()
  }
  
  currentChat.value.messages.push(userMessage)
  inputMessage.value = ''
  
  try {
    const model = props.models.find(m => m.id === currentChat.value.modelId)
    if (!model) throw new Error('未找到选择的模型')

    // 获取对话历史
    const context = []
    
    // 如果选择了提示词，添加为 system 消息
    if (currentChat.value.promptId) {
      const prompt = props.prompts.find(p => p.id === currentChat.value.promptId)
      if (prompt) {
        context.push({
          role: 'system',
          content: prompt.template
        })
      }
    }

    // 添加历史消息
    context.push(...currentChat.value.messages
      .filter(msg => !msg.isStreaming) // 过滤掉正在流式输出的消息
      .map(msg => ({
        role: msg.role === 'user' ? 'user' : 'assistant',
        content: msg.content
      })))
    context.pop() // 移除最后一条消息，因为它会作为当前消息发送

    // 处理关键词检测
    let processedContent = userMessage.content
    if (currentChat.value.enableKeywords) {
      const keywords = allKeywords.value.filter(keyword => 
        userMessage.content.toLowerCase().includes(keyword.name.toLowerCase())
      )

      if (keywords.length > 0) {
        let keywordsContext = '检测到以下相关内容:\n\n'
        
        for (const keyword of keywords) {
          const content = getKeywordContent(keyword)
          if (content) {
            keywordsContext += `【${keyword.name}】:\n${content}\n\n`
          }
        }
        
        // 将关键词内容添加到用户消息前面
        processedContent = keywordsContext + '用户问题:\n' + userMessage.content
      }
    }
    
    // 调用 API 获取回复 (现在会处理流式输出)
    await sendToModel(model, processedContent, context)
    
    // 不需要再添加消息，因为流式输出已经添加了
    saveSessions()

  } catch (error) {
    console.error('发送失败:', error)
    // 移除最后一条用户消息
    currentChat.value.messages = currentChat.value.messages.filter(m => m.id !== userMessage.id)
    alert(error.message)
  }
}
// ... existing code ...

// 修改消息显示部分，添加打字机效果
<div class="messages" ref="messagesContainer">
  <div 
    v-for="msg in currentChat.messages" 
    :key="msg.id"
    :class="['message', msg.role, { 'streaming': msg.isStreaming }]"
  >
    <div class="message-header">
      <span class="role-badge">{{ msg.role === 'user' ? '我' : 'AI' }}</span>
      <span class="message-time">{{ formatTime(msg.timestamp) }}</span>
      <div class="message-actions">
        <button class="icon-btn" @click="editMessage(msg)" title="编辑" v-if="!msg.isStreaming">
          <i class="fas fa-edit"></i>
        </button>
        <button class="icon-btn" @click="resendMessage(msg)" v-if="msg.role === 'user'" title="重新发送">
          <i class="fas fa-redo"></i>
        </button>
        <button class="icon-btn delete" @click="deleteMessage(msg.id)" title="删除" v-if="!msg.isStreaming">
          <i class="fas fa-trash"></i>
        </button>
        <button 
          class="icon-btn split-btn" 
          @click="splitMessage(msg)" 
          v-if="msg.role === 'assistant' && !msg.isStreaming"
          title="拆分内容"
        >
          <i class="fas fa-cut"></i>
        </button>
      </div>
    </div>
    <div 
      class="message-content" 
      v-html="formatMessage(msg.content)"
      v-if="!msg.isEditing"
    ></div>
    <textarea
      v-else
      v-model="msg.editContent"
      class="edit-textarea"
      @keydown.enter.exact.prevent="saveMessageEdit(msg)"
      @keydown.esc="cancelMessageEdit(msg)"
    ></textarea>
    <div class="edit-actions" v-if="msg.isEditing">
      <button class="small-btn" @click="saveMessageEdit(msg)">保存</button>
      <button class="small-btn" @click="cancelMessageEdit(msg)">取消</button>
    </div>
    <div class="streaming-indicator" v-if="msg.isStreaming">
      <span class="dot"></span>
      <span class="dot"></span>
      <span class="dot"></span>
    </div>
  </div>
</div>
// ... existing code ...
```

You'll also need to add some CSS for the streaming effect. Add these styles to your `chatView.css` file:

```css:src/styles/chatView.css
/* 流式输出相关样式 */
.message.streaming {
  position: relative;
}

.streaming-indicator {
  display: flex;
  margin-top: 5px;
  align-items: center;
}

.streaming-indicator .dot {
  display: inline-block;
  width: 8px;
  height: 8px;
  margin: 0 2px;
  background-color: #888;
  border-radius: 50%;
  opacity: 0.6;
  animation: pulse 1.5s infinite ease-in-out;
}

.streaming-indicator .dot:nth-child(2) {
  animation-delay: 0.2s;
}

.streaming-indicator .dot:nth-child(3) {
  animation-delay: 0.4s;
}

@keyframes pulse {
  0%, 100% {
    opacity: 0.4;
    transform: scale(0.8);
  }
  50% {
    opacity: 1;
    transform: scale(1.2);
  }
}

/* 为流式输出中的消息添加打字机光标效果 */
.message.streaming .message-content::after {
  content: '|';
  display: inline-block;
  animation: blink 1s infinite;
  font-weight: bold;
}

@keyframes blink {
  0%, 100% { opacity: 1; }
  50% { opacity: 0; }
}
```

These changes implement streaming output for OpenAI-compatible APIs (including OpenAI, StepFun, Mistral, and custom providers). For Gemini, which doesn't support streaming in the same way, we maintain the regular request pattern but still show the message appearing incrementally.

The implementation:
1. Creates a temporary message with an `isStreaming` flag when a request starts
2. Processes the streaming response chunk by chunk, updating the message content
3. Shows a typing indicator with animated dots while streaming
4. Adds a blinking cursor effect to make it look like the AI is typing
5. Disables edit/delete/split buttons while a message is streaming

This creates a more engaging and responsive chat experience for users.

---

_**User**_

有加载样式，但没有正确流式输出成功

---

_**Assistant**_

我看到问题了，您的代码中已经添加了流式输出的相关代码，但可能在实际处理流式响应时存在一些问题。让我们修复一下流式输出的实现：

```javascript:src/components/ChatView.vue
// 修改调用模型 API
const sendToModel = async (model, content, context = []) => {
  try {
    let response
    let result

    // 检查必要的参数
    if (!model?.provider || !model?.apiUrl || !model?.apiKey) {
      throw new Error('模型配置不完整')
    }

    console.log('Sending request to model:', model.provider)

    // 构建基础请求配置
    const baseHeaders = {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${model.apiKey}`
    }

    // 构建基础消息体
    const baseMessages = [...context, { role: 'user', content }]

    // 创建一个临时消息用于流式输出
    const tempMessage = {
      id: Date.now(),
      role: 'assistant',
      content: '',
      timestamp: new Date(),
      isStreaming: true
    }
    currentChat.value.messages.push(tempMessage)
    await nextTick()
    scrollToBottom()

    // 根据不同提供商处理请求
    switch (model.provider) {
      case 'openai':
      case 'stepfun':
      case 'mistral':
        try {
          response = await fetch(`${model.apiUrl}/v1/chat/completions`, {
            method: 'POST',
            headers: baseHeaders,
            body: JSON.stringify({
              model: model.modelId,
              messages: baseMessages,
              max_tokens: Number(model.maxTokens),
              temperature: Number(model.temperature),
              stream: true // 启用流式输出
            })
          })
          
          if (!response.ok) {
            const errorData = await response.json().catch(() => ({}))
            throw new Error(`请求失败: ${response.status} ${errorData.error?.message || ''}`)
          }
          
          const reader = response.body.getReader()
          const decoder = new TextDecoder("utf-8")
          
          let buffer = ''
          
          while (true) {
            const { done, value } = await reader.read()
            if (done) break
            
            // 解码新的块并添加到缓冲区
            buffer += decoder.decode(value, { stream: true })
            
            // 处理缓冲区中的所有完整行
            const lines = buffer.split('\n')
            buffer = lines.pop() || '' // 保留最后一个不完整的行
            
            for (const line of lines) {
              const trimmedLine = line.trim()
              if (!trimmedLine || !trimmedLine.startsWith('data: ')) continue
              
              const data = trimmedLine.substring(6)
              if (data === '[DONE]') continue
              
              try {
                const parsed = JSON.parse(data)
                const content = parsed.choices[0]?.delta?.content || ''
                if (content) {
                  tempMessage.content += content
                  await nextTick()
                  scrollToBottom()
                }
              } catch (e) {
                console.error('解析流数据失败:', e, data)
              }
            }
          }
          
          // 处理最后可能的数据
          if (buffer.trim()) {
            const trimmedLine = buffer.trim()
            if (trimmedLine.startsWith('data: ') && trimmedLine.substring(6) !== '[DONE]') {
              try {
                const parsed = JSON.parse(trimmedLine.substring(6))
                const content = parsed.choices[0]?.delta?.content || ''
                if (content) {
                  tempMessage.content += content
                }
              } catch (e) {
                console.error('解析最后的流数据失败:', e, trimmedLine)
              }
            }
          }
        } catch (error) {
          console.error('流式处理错误:', error)
          throw error
        } finally {
          // 流式输出完成后
          tempMessage.isStreaming = false
          await nextTick()
          scrollToBottom()
          saveSessions()
        }
        return tempMessage.content
        
      case 'custom': // 直接使用提供的 URL，不附加路径
        try {
          response = await fetch(model.apiUrl, {
            method: 'POST',
            headers: baseHeaders,
            body: JSON.stringify({
              model: model.modelId,
              messages: baseMessages,
              max_tokens: Number(model.maxTokens),
              temperature: Number(model.temperature),
              stream: true // 启用流式输出
            })
          })
          
          if (!response.ok) {
            const errorData = await response.json().catch(() => ({}))
            throw new Error(`请求失败: ${response.status} ${errorData.error?.message || ''}`)
          }
          
          const customReader = response.body.getReader()
          const customDecoder = new TextDecoder("utf-8")
          
          let customBuffer = ''
          
          while (true) {
            const { done, value } = await customReader.read()
            if (done) break
            
            // 解码新的块并添加到缓冲区
            customBuffer += customDecoder.decode(value, { stream: true })
            
            // 处理缓冲区中的所有完整行
            const lines = customBuffer.split('\n')
            customBuffer = lines.pop() || '' // 保留最后一个不完整的行
            
            for (const line of lines) {
              const trimmedLine = line.trim()
              if (!trimmedLine || !trimmedLine.startsWith('data: ')) continue
              
              const data = trimmedLine.substring(6)
              if (data === '[DONE]') continue
              
              try {
                const parsed = JSON.parse(data)
                const content = parsed.choices[0]?.delta?.content || ''
                if (content) {
                  tempMessage.content += content
                  await nextTick()
                  scrollToBottom()
                }
              } catch (e) {
                console.error('解析流数据失败:', e, data)
              }
            }
          }
          
          // 处理最后可能的数据
          if (customBuffer.trim()) {
            const trimmedLine = customBuffer.trim()
            if (trimmedLine.startsWith('data: ') && trimmedLine.substring(6) !== '[DONE]') {
              try {
                const parsed = JSON.parse(trimmedLine.substring(6))
                const content = parsed.choices[0]?.delta?.content || ''
                if (content) {
                  tempMessage.content += content
                }
              } catch (e) {
                console.error('解析最后的流数据失败:', e, trimmedLine)
              }
            }
          }
        } catch (error) {
          console.error('流式处理错误:', error)
          throw error
        } finally {
          // 流式输出完成后
          tempMessage.isStreaming = false
          await nextTick()
          scrollToBottom()
          saveSessions()
        }
        return tempMessage.content

      case 'gemini':
        const contents = context.map(msg => ({
          parts: [{ text: msg.content }],
          role: msg.role === 'user' ? 'user' : 'model'
        }))
        
        contents.push({
          parts: [{ text: content }],
          role: 'user'
        })

        // 模拟 Gemini 的流式输出
        try {
          response = await fetch(`${model.apiUrl}?key=${model.apiKey}`, {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json'
            },
            body: JSON.stringify({
              contents,
              generationConfig: {
                temperature: Number(model.temperature),
                maxOutputTokens: Number(model.maxTokens)
              }
            })
          })
          
          if (!response.ok) {
            const errorData = await response.json().catch(() => ({}))
            throw new Error(`请求失败: ${response.status} ${errorData.error?.message || ''}`)
          }
          
          result = await response.json()
          const fullText = result.candidates[0].content.parts[0].text
          
          // 模拟流式输出，每次添加一小部分文本
          const chunkSize = 5 // 每次添加的字符数
          for (let i = 0; i < fullText.length; i += chunkSize) {
            const chunk = fullText.substring(i, i + chunkSize)
            tempMessage.content += chunk
            await new Promise(resolve => setTimeout(resolve, 10)) // 添加小延迟
            await nextTick()
            scrollToBottom()
          }
        } catch (error) {
          console.error('Gemini 处理错误:', error)
          throw error
        } finally {
          tempMessage.isStreaming = false
          await nextTick()
          scrollToBottom()
          saveSessions()
        }
        return tempMessage.content

      default:
        throw new Error(`不支持的模型提供商: ${model.provider}`)
    }

  } catch (error) {
    console.error('API 请求错误:', error)
    // 移除临时消息
    currentChat.value.messages = currentChat.value.messages.filter(m => !m.isStreaming)
    throw new Error(`发送失败: ${error.message}`)
  }
}
```

这个修改版本的 `sendToModel` 函数做了以下改进：

1. 添加了更健壮的错误处理和 try/catch/finally 块
2. 改进了流式数据的处理方式，使用缓冲区来处理可能被分割的 JSON 数据
3. 为 Gemini API（不原生支持流式输出）添加了模拟流式输出的功能
4. 确保在每次更新内容后都调用 `nextTick()` 和 `scrollToBottom()`
5. 在流式输出开始和结束时都确保滚动到底部
6. 使用 `{ stream: true }` 选项进行解码，这对于处理流式数据很重要

这些修改应该能解决流式输出不正常工作的问题。如果您仍然遇到问题，可能需要检查：

1. 您使用的 API 是否正确支持流式输出
2. 网络连接是否稳定
3. 浏览器控制台中是否有任何错误信息

---

_**User**_

[vite] connecting...
client:865 [vite] connected.
client:836 [vite] hot updated: /src/components/ChatView.vue
[Violation]Forced reflow while executing JavaScript took 36ms
ChatView.vue:446 Sending request to model: custom 没有报错显示就是没有输出任何字，刷新后字数会正确输出

---

_**Assistant**_



看起来您的代码连接成功了，但流式输出仍然没有正常工作。问题可能是在处理流式响应数据的方式上。让我们进一步调整流式输出的处理逻辑，并添加更多的调试信息来帮助诊断问题：

```javascript:src/components/ChatView.vue
// 修改调用模型 API
const sendToModel = async (model, content, context = []) => {
  try {
    let response
    let result

    // 检查必要的参数
    if (!model?.provider || !model?.apiUrl || !model?.apiKey) {
      throw new Error('模型配置不完整')
    }

    console.log('Sending request to model:', model.provider)

    // 构建基础请求配置
    const baseHeaders = {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${model.apiKey}`
    }

    // 构建基础消息体
    const baseMessages = [...context, { role: 'user', content }]

    // 创建一个临时消息用于流式输出
    const tempMessage = {
      id: Date.now(),
      role: 'assistant',
      content: '',
      timestamp: new Date(),
      isStreaming: true
    }
    currentChat.value.messages.push(tempMessage)
    await nextTick()
    scrollToBottom()

    // 根据不同提供商处理请求
    switch (model.provider) {
      case 'openai':
      case 'stepfun':
      case 'mistral':
      case 'custom': // 将 custom 和其他 OpenAI 兼容 API 一起处理
        try {
          // 确定正确的 API URL
          const apiUrl = model.provider === 'custom' 
            ? model.apiUrl 
            : `${model.apiUrl}/v1/chat/completions`
          
          console.log('Sending streaming request to:', apiUrl)
          
          response = await fetch(apiUrl, {
            method: 'POST',
            headers: baseHeaders,
            body: JSON.stringify({
              model: model.modelId,
              messages: baseMessages,
              max_tokens: Number(model.maxTokens),
              temperature: Number(model.temperature),
              stream: true // 启用流式输出
            })
          })
          
          if (!response.ok) {
            const errorData = await response.json().catch(() => ({}))
            throw new Error(`请求失败: ${response.status} ${errorData.error?.message || ''}`)
          }
          
          console.log('Stream response received, starting to process...')
          
          // 使用更简单的方法处理流
          const reader = response.body.getReader()
          const decoder = new TextDecoder()
          
          let buffer = ''
          let receivedAnyContent = false
          
          while (true) {
            const { done, value } = await reader.read()
            
            if (done) {
              console.log('Stream completed')
              break
            }
            
            // 解码新的块
            const chunk = decoder.decode(value, { stream: true })
            console.log('Received chunk:', chunk.length, 'bytes')
            
            // 将新块添加到缓冲区
            buffer += chunk
            
            // 处理缓冲区中的所有完整行
            const lines = buffer.split('\n')
            // 保留最后一个可能不完整的行
            buffer = lines.pop() || ''
            
            for (const line of lines) {
              if (!line.trim()) continue
              
              if (line.includes('data:')) {
                const dataPart = line.substring(line.indexOf('data:') + 5).trim()
                
                if (dataPart === '[DONE]') {
                  console.log('Received [DONE] signal')
                  continue
                }
                
                try {
                  const parsed = JSON.parse(dataPart)
                  const content = parsed.choices?.[0]?.delta?.content || ''
                  
                  if (content) {
                    receivedAnyContent = true
                    console.log('Received content:', content)
                    tempMessage.content += content
                    await nextTick()
                    scrollToBottom()
                  }
                } catch (e) {
                  console.error('解析流数据失败:', e, dataPart)
                }
              }
            }
          }
          
          // 如果没有收到任何内容，可能是流格式不同，尝试非流式请求
          if (!receivedAnyContent) {
            console.log('No streaming content received, falling back to non-streaming request')
            await fallbackToNonStreaming(model, baseMessages, tempMessage)
          }
          
        } catch (error) {
          console.error('流式处理错误:', error)
          // 尝试回退到非流式请求
          try {
            console.log('Error in streaming, falling back to non-streaming request')
            await fallbackToNonStreaming(model, baseMessages, tempMessage)
          } catch (fallbackError) {
            throw error // 如果回退也失败，抛出原始错误
          }
        } finally {
          // 流式输出完成后
          tempMessage.isStreaming = false
          await nextTick()
          scrollToBottom()
          saveSessions()
        }
        return tempMessage.content

      case 'gemini':
        const contents = context.map(msg => ({
          parts: [{ text: msg.content }],
          role: msg.role === 'user' ? 'user' : 'model'
        }))
        
        contents.push({
          parts: [{ text: content }],
          role: 'user'
        })

        // 模拟 Gemini 的流式输出
        try {
          console.log('Sending request to Gemini API')
          response = await fetch(`${model.apiUrl}?key=${model.apiKey}`, {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json'
            },
            body: JSON.stringify({
              contents,
              generationConfig: {
                temperature: Number(model.temperature),
                maxOutputTokens: Number(model.maxTokens)
              }
            })
          })
          
          if (!response.ok) {
            const errorData = await response.json().catch(() => ({}))
            throw new Error(`请求失败: ${response.status} ${errorData.error?.message || ''}`)
          }
          
          result = await response.json()
          console.log('Received Gemini response, simulating streaming')
          const fullText = result.candidates[0].content.parts[0].text
          
          // 模拟流式输出，每次添加一小部分文本
          const chunkSize = 5 // 每次添加的字符数
          for (let i = 0; i < fullText.length; i += chunkSize) {
            const chunk = fullText.substring(i, i + chunkSize)
            tempMessage.content += chunk
            await new Promise(resolve => setTimeout(resolve, 10)) // 添加小延迟
            await nextTick()
            scrollToBottom()
          }
        } catch (error) {
          console.error('Gemini 处理错误:', error)
          throw error
        } finally {
          tempMessage.isStreaming = false
          await nextTick()
          scrollToBottom()
          saveSessions()
        }
        return tempMessage.content

      default:
        throw new Error(`不支持的模型提供商: ${model.provider}`)
    }

  } catch (error) {
    console.error('API 请求错误:', error)
    // 移除临时消息
    currentChat.value.messages = currentChat.value.messages.filter(m => !m.isStreaming)
    throw new Error(`发送失败: ${error.message}`)
  }
}

// 添加回退到非流式请求的辅助函数
const fallbackToNonStreaming = async (model, messages, tempMessage) => {
  console.log('Attempting non-streaming fallback request')
  
  // 确定正确的 API URL
  const apiUrl = model.provider === 'custom' 
    ? model.apiUrl 
    : `${model.apiUrl}/v1/chat/completions`
  
  const response = await fetch(apiUrl, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${model.apiKey}`
    },
    body: JSON.stringify({
      model: model.modelId,
      messages: messages,
      max_tokens: Number(model.maxTokens),
      temperature: Number(model.temperature),
      stream: false // 禁用流式输出
    })
  })
  
  if (!response.ok) {
    const errorData = await response.json().catch(() => ({}))
    throw new Error(`请求失败: ${response.status} ${errorData.error?.message || ''}`)
  }
  
  const result = await response.json()
  console.log('Non-streaming fallback successful')
  
  // 模拟流式输出
  const fullText = result.choices[0].message.content
  
  // 模拟流式输出，每次添加一小部分文本
  const chunkSize = 5 // 每次添加的字符数
  for (let i = 0; i < fullText.length; i += chunkSize) {
    const chunk = fullText.substring(i, i + chunkSize)
    tempMessage.content += chunk
    await new Promise(resolve => setTimeout(resolve, 10)) // 添加小延迟
    await nextTick()
    scrollToBottom()
  }
  
  return fullText
}
```

这个更新版本的代码做了以下改进：

1. 添加了更多的调试日志，帮助诊断问题
2. 简化了流式数据处理逻辑，使其更健壮
3. 添加了回退机制 - 如果流式请求没有返回内容或出错，会自动尝试非流式请求
4. 将 `custom` 提供商与其他 OpenAI 兼容 API 一起处理，但正确处理 URL 差异
5. 改进了数据行的解析方式，更灵活地处理不同格式的流式响应
6. 添加了检测是否收到任何内容的逻辑，如果没有则回退

这些改进应该能解决流式输出的问题。如果仍然不工作，请检查浏览器控制台中的日志，看看是否有更具体的错误信息或者是否成功接收到了数据块但没有正确处理。

---

_**User**_

Sending request to model: custom
ChatView.vue:481 Sending streaming request to: https://open.bigmodel.cn/api/paas/v4/chat/completions
ChatView.vue:500 Stream response received, starting to process...
ChatView.vue:519 Received chunk: 472 bytes
ChatView.vue:546 Received content: 数字
ChatView.vue:546 Received content:  
ChatView.vue:546 Received content: 3
ChatView.vue:519 Received chunk: 631 bytes
ChatView.vue:546 Received content: 12
ChatView.vue:546 Received content: 3
ChatView.vue:546 Received content: 31
ChatView.vue:546 Received content: 23
ChatView.vue:519 Received chunk: 631 bytes
ChatView.vue:546 Received content:  是
ChatView.vue:546 Received content: 一个
ChatView.vue:546 Received content: 八
ChatView.vue:546 Received content: 位数
ChatView.vue:519 Received chunk: 632 bytes
ChatView.vue:546 Received content: ，
ChatView.vue:546 Received content: 以下
ChatView.vue:546 Received content: 是一些
ChatView.vue:546 Received content: 关于
ChatView.vue:519 Received chunk: 474 bytes
ChatView.vue:546 Received content: 它的
ChatView.vue:546 Received content: 数学
ChatView.vue:546 Received content: 特性
ChatView.vue:519 Received chunk: 637 bytes
ChatView.vue:546 Received content: ：

-
ChatView.vue:546 Received content:  **
ChatView.vue:546 Received content: 因子
ChatView.vue:546 Received content: **
ChatView.vue:519 Received chunk: 630 bytes
ChatView.vue:546 Received content: ：
ChatView.vue:546 Received content: 要
ChatView.vue:546 Received content: 找出
ChatView.vue:546 Received content: 所有
ChatView.vue:519 Received chunk: 633 bytes
ChatView.vue:546 Received content: 因子
ChatView.vue:546 Received content: ，
ChatView.vue:546 Received content: 我们需要
ChatView.vue:546 Received content: 分解
ChatView.vue:519 Received chunk: 629 bytes
ChatView.vue:546 Received content: 它的
ChatView.vue:546 Received content: 质
ChatView.vue:546 Received content: 因
ChatView.vue:546 Received content: 数
ChatView.vue:519 Received chunk: 157 bytes
ChatView.vue:546 Received content: 。
ChatView.vue:519 Received chunk: 158 bytes
ChatView.vue:546 Received content: 不过
ChatView.vue:519 Received chunk: 157 bytes
ChatView.vue:546 Received content: ，
ChatView.vue:519 Received chunk: 633 bytes
ChatView.vue:546 Received content: 我可以
ChatView.vue:546 Received content: 直接
ChatView.vue:546 Received content: 告诉你
ChatView.vue:546 Received content: ，
ChatView.vue:519 Received chunk: 630 bytes
ChatView.vue:546 Received content: 3
ChatView.vue:546 Received content: 12
ChatView.vue:546 Received content: 3
ChatView.vue:546 Received content: 31
ChatView.vue:519 Received chunk: 631 bytes
ChatView.vue:546 Received content: 23
ChatView.vue:546 Received content:  可以
ChatView.vue:546 Received content: 被
ChatView.vue:546 Received content:  
ChatView.vue:519 Received chunk: 629 bytes
ChatView.vue:546 Received content: 7
ChatView.vue:546 Received content: 、
ChatView.vue:546 Received content: 13
ChatView.vue:546 Received content: 、
ChatView.vue:519 Received chunk: 631 bytes
ChatView.vue:546 Received content: 19
ChatView.vue:546 Received content:  和
ChatView.vue:546 Received content:  
ChatView.vue:546 Received content: 37
ChatView.vue:519 Received chunk: 472 bytes
ChatView.vue:546 Received content:  整
ChatView.vue:546 Received content: 除
ChatView.vue:546 Received content: ，
ChatView.vue:519 Received chunk: 632 bytes
ChatView.vue:546 Received content: 因此
ChatView.vue:546 Received content: 它的
ChatView.vue:546 Received content: 因子
ChatView.vue:546 Received content: 包括
ChatView.vue:519 Received chunk: 632 bytes
ChatView.vue:546 Received content: 这些
ChatView.vue:546 Received content: 数
ChatView.vue:546 Received content: 以及
ChatView.vue:546 Received content: 它们的
ChatView.vue:519 Received chunk: 474 bytes
ChatView.vue:546 Received content: 乘
ChatView.vue:546 Received content: 积
ChatView.vue:546 Received content: 。
-
ChatView.vue:519 Received chunk: 631 bytes
ChatView.vue:546 Received content:  **
ChatView.vue:546 Received content: 素
ChatView.vue:546 Received content: 数
ChatView.vue:546 Received content: 与否
ChatView.vue:519 Received chunk: 630 bytes
ChatView.vue:546 Received content: **
ChatView.vue:546 Received content: ：
ChatView.vue:546 Received content: 3
ChatView.vue:546 Received content: 12
ChatView.vue:519 Received chunk: 315 bytes
ChatView.vue:546 Received content: 3
ChatView.vue:546 Received content: 31
ChatView.vue:519 Received chunk: 631 bytes
ChatView.vue:546 Received content: 23
ChatView.vue:546 Received content:  不是
ChatView.vue:546 Received content: 素
ChatView.vue:546 Received content: 数
ChatView.vue:519 Received chunk: 631 bytes
ChatView.vue:546 Received content: ，
ChatView.vue:546 Received content: 因为它
ChatView.vue:546 Received content: 有
ChatView.vue:546 Received content: 除了
ChatView.vue:519 Received chunk: 629 bytes
ChatView.vue:546 Received content:  
ChatView.vue:546 Received content: 1
ChatView.vue:546 Received content:  和
ChatView.vue:546 Received content: 它
ChatView.vue:519 Received chunk: 475 bytes
ChatView.vue:546 Received content: 本身
ChatView.vue:546 Received content: 以外的
ChatView.vue:546 Received content: 因子
ChatView.vue:519 Received chunk: 319 bytes
ChatView.vue:546 Received content: 。
-
ChatView.vue:546 Received content:  **
ChatView.vue:519 Received chunk: 630 bytes
ChatView.vue:546 Received content: 数字
ChatView.vue:546 Received content: 和
ChatView.vue:546 Received content: **
ChatView.vue:546 Received content: ：
ChatView.vue:519 Received chunk: 472 bytes
ChatView.vue:546 Received content: 3
ChatView.vue:546 Received content: 12
ChatView.vue:546 Received content: 3
ChatView.vue:519 Received chunk: 632 bytes
ChatView.vue:546 Received content: 31
ChatView.vue:546 Received content: 23
ChatView.vue:546 Received content:  的
ChatView.vue:546 Received content: 数字
ChatView.vue:519 Received chunk: 157 bytes
ChatView.vue:546 Received content: 和
ChatView.vue:519 Received chunk: 629 bytes
ChatView.vue:546 Received content: 是
ChatView.vue:546 Received content:  
ChatView.vue:546 Received content: 3
ChatView.vue:546 Received content:  +
ChatView.vue:519 Received chunk: 629 bytes
ChatView.vue:546 Received content:  
ChatView.vue:546 Received content: 1
ChatView.vue:546 Received content:  +
ChatView.vue:546 Received content:  
ChatView.vue:519 Received chunk: 629 bytes
ChatView.vue:546 Received content: 2
ChatView.vue:546 Received content:  +
ChatView.vue:546 Received content:  
ChatView.vue:546 Received content: 3
ChatView.vue:519 Received chunk: 472 bytes
ChatView.vue:546 Received content:  +
ChatView.vue:546 Received content:  
ChatView.vue:546 Received content: 3
ChatView.vue:519 Received chunk: 158 bytes
ChatView.vue:546 Received content:  +
ChatView.vue:519 Received chunk: 629 bytes
ChatView.vue:546 Received content:  
ChatView.vue:546 Received content: 1
ChatView.vue:546 Received content:  +
ChatView.vue:546 Received content:  
ChatView.vue:519 Received chunk: 629 bytes
ChatView.vue:546 Received content: 2
ChatView.vue:546 Received content:  +
ChatView.vue:546 Received content:  
ChatView.vue:546 Received content: 3
ChatView.vue:519 Received chunk: 473 bytes
ChatView.vue:546 Received content:  =
ChatView.vue:546 Received content:  
ChatView.vue:546 Received content: 17
ChatView.vue:519 Received chunk: 635 bytes
ChatView.vue:546 Received content: 。
-
ChatView.vue:546 Received content:  **
ChatView.vue:546 Received content: 数字
ChatView.vue:546 Received content: 特性
ChatView.vue:519 Received chunk: 631 bytes
ChatView.vue:546 Received content: **
ChatView.vue:546 Received content: ：
ChatView.vue:546 Received content: 这个
ChatView.vue:546 Received content: 数字
ChatView.vue:519 Received chunk: 633 bytes
ChatView.vue:546 Received content: 没有
ChatView.vue:546 Received content: 明显的
ChatView.vue:546 Received content: 特殊
ChatView.vue:546 Received content: 数学
ChatView.vue:519 Received chunk: 630 bytes
ChatView.vue:546 Received content: 特性
ChatView.vue:546 Received content: ，
ChatView.vue:546 Received content: 比如
ChatView.vue:546 Received content: 它
ChatView.vue:519 Received chunk: 633 bytes
ChatView.vue:546 Received content: 不是一个
ChatView.vue:546 Received content: 完全
ChatView.vue:546 Received content: 平方
ChatView.vue:546 Received content: 数
ChatView.vue:519 Received chunk: 631 bytes
ChatView.vue:546 Received content: ，
ChatView.vue:546 Received content: 也不是
ChatView.vue:546 Received content: 一个
ChatView.vue:546 Received content: 回
ChatView.vue:519 Received chunk: 478 bytes
ChatView.vue:546 Received content: 文
ChatView.vue:546 Received content: 数
ChatView.vue:546 Received content: 。

如果你
ChatView.vue:519 Received chunk: 632 bytes
ChatView.vue:546 Received content: 想要
ChatView.vue:546 Received content: 更
ChatView.vue:546 Received content: 详细的
ChatView.vue:546 Received content: 因子
ChatView.vue:519 Received chunk: 631 bytes
ChatView.vue:546 Received content: 分解
ChatView.vue:546 Received content: 或者
ChatView.vue:546 Received content: 有
ChatView.vue:546 Received content: 其他
ChatView.vue:519 Received chunk: 633 bytes
ChatView.vue:546 Received content: 关于
ChatView.vue:546 Received content: 这个
ChatView.vue:546 Received content: 数字
ChatView.vue:546 Received content: 的问题
ChatView.vue:519 Received chunk: 630 bytes
ChatView.vue:546 Received content: ，
ChatView.vue:546 Received content: 请
ChatView.vue:546 Received content: 告诉我
ChatView.vue:546 Received content: ！
ChatView.vue:519 Received chunk: 270 bytes
ChatView.vue:546 Received content: 😊
ChatView.vue:536 Received [DONE] signal
ChatView.vue:513 Stream completed后台输出了，但是前端没有渲染，一个字都没显示

---

_**Assistant**_



---



---

